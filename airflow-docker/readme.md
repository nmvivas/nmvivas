<h1 align="center">Docs Airflow en Docker</h1>
<h3 align="center">ETL - Data Mining (para principiantes)</h3>

<h4 align="left">Herramientas Utilizadas</h4>

- üìå Visual Studio Code

- üìå Python 3.11.3 <a href="https://www.python.org" target="_blank" rel="noreferrer"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg" alt="python" width="20" height="20"/> </a>
  
- üìå Docker Desktop 23.0.5
- üìå Docker compose v2.17.3
- üìå Dbeaver 

<h3 align="left">--- Paso a Paso --- </h3>
<hr>
<h3 align="left">‚û§ Instalaci√≥n</h3>
1. Instala Visual Studio Code en tu m√°quina.
   - Descarga el instalador desde [https://code.visualstudio.com](https://code.visualstudio.com) y sigue las instrucciones de instalaci√≥n para tu sistema operativo.
   
2. Instala Python 3.11.3.
   - Ve al sitio web oficial de Python en [https://www.python.org](https://www.python.org) y descarga el instalador correspondiente a tu sistema operativo.
   - Sigue las instrucciones de instalaci√≥n para configurar Python en tu m√°quina.
     
3. Configura Docker 23.0.5.
   - Visita [https://www.docker.com](https://www.docker.com) y descarga Docker para tu sistema operativo.
   - Sigue las instrucciones de instalaci√≥n proporcionadas por Docker para configurar Docker en tu m√°quina.
     
4. Instala Docker Compose.
   - Ve al sitio web oficial de Docker Compose en [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/) y sigue 
    las instrucciones para instalar Docker Compose en tu sistema operativo.4. Instala Docker Compose.

<hr> 

<h3 align="left">‚û§ Configuraci√≥n de docker compose</h3>
El archivo se encuentra en el repositorio con el nombre ***docker-compose.yaml***

Este es un archivo de configuraci√≥n de Docker Compose que define los servicios necesarios para ejecutar Apache Airflow en contenedores Docker.

Est√° estructurado de la siguiente manera: 

### Versi√≥n
La versi√≥n utilizada en este archivo de configuraci√≥n de Docker Compose es `3.7`. Aseg√∫rate de tener instalada la versi√≥n adecuada de Docker Compose para que funcione correctamente.
### Servicios
El archivo de configuraci√≥n define los siguientes servicios:

Este c√≥digo de Docker Compose define dos servicios: webserver y scheduler. 

El servicio webserver utiliza la imagen apache/airflow:2.6.2 y se reinicia siempre.
- Est√° configurado para usar un executor local, una conexi√≥n de base de datos PostgreSQL, y se ha proporcionado una clave Fernet para encriptar los datos.
- Adem√°s, se crea un usuario de Airflow con el rol de administrador.
- El servicio webserver expone el puerto 8080 y monta vol√∫menes para los DAGs, logs y plugins de Airflow.

El servicio scheduler depende del servicio webserver y utiliza la misma imagen apache/airflow:2.6.2. 
- Est√° configurado para utilizar el mismo executor local y la misma conexi√≥n de base de datos que el servicio webserver.
- Monta los mismos vol√∫menes y realiza una inicializaci√≥n de la base de datos de Airflow antes de ejecutar el scheduler.

  ***Recuerda ajustar las variables de entorno, los vol√∫menes y los puertos seg√∫n tus necesidades. Adem√°s, aseg√∫rate de tener la red apachetl correctamente configurada en tu entorno Docker.***

<hr>

<h3 align="left">‚û§ Configuracion de la base de Datos</h3>

Para la configuraci√≥n de la base de datos, se utiliz√≥ DBeaver, una herramienta de gesti√≥n de bases de datos. A continuaci√≥n, se detallan los pasos para configurar la conexi√≥n con PostgreSQL y crear una base de datos para Airflow.

1. Instala DBeaver en tu m√°quina.
   - Descarga el instalador de DBeaver desde [https://dbeaver.io](https://dbeaver.io) y sigue las instrucciones de instalaci√≥n para tu sistema operativo.

2. Abre DBeaver y crea una nueva conexi√≥n.
   - Haz clic en "Nuevo" para crear una nueva conexi√≥n a la base de datos.
   - Selecciona "PostgreSQL" como el tipo de base de datos.

3. Configura la conexi√≥n a PostgreSQL.
   - Ingresa los siguientes detalles de conexi√≥n:
     - Host: `localhost` (o la direcci√≥n IP del contenedor de Docker donde se encuentra PostgreSQL)
     - Puerto: `5432`
     - Base de datos: `nath`
     - Usuario: `airflow`
     - Contrase√±a: (la contrase√±a configurada para el usuario `airflow`)
   - Haz clic en "Probar conexi√≥n" para verificar que la conexi√≥n se establece correctamente.

4. Crea la base de datos para Airflow.
   - Una vez que la conexi√≥n se haya establecido correctamente, haz clic con el bot√≥n derecho en la conexi√≥n en DBeaver y selecciona "Crear base de datos".
   - Asigna el nombre "nath" a la base de datos y confirma la creaci√≥n.

5. Verifica la conexi√≥n y la base de datos.
   - Expande la conexi√≥n en DBeaver y verifica que la base de datos "nath" est√© presente.

Con estos pasos, has configurado correctamente la conexi√≥n a PostgreSQL y creado la base de datos (en mi caso) "nath" para Airflow. Aseg√∫rate de utilizar la direcci√≥n IP correcta y la contrase√±a adecuada para el usuario `airflow` seg√∫n tu entorno.

***_Recuerda que Airflow utiliza esta base de datos para almacenar la metadatos y configuraciones relacionadas con los DAGs y tareas._***

<hr> 
Una vez que ya los contenderores de docker esten funcional y pueda acceder a la interfaz de airflow. Ver√° algo as√≠ 
....
<h3 align="left">‚û§ Creaci√≥n de un DAG con procesos ETL</h3>
Este DAG utiliza un conjunto de datos en formato .csv que contiene estad√≠sticas sobre cr√≠menes en la India. El conjunto de datos se llama "20_Victims_of_rape.csv" y se puede encontrar en la p√°gina https://www.kaggle.com/code/himanshukamal/victims-of-rape/input?select=20_Victims_of_rape.csv.


El objetivo de este DAG es realizar un proceso ETL (Extract - Transform - Load) para cargar el archivo CSV en una base de datos PostgreSQL.

El c√≥digo completo del DAG se encuentra en la carpeta "dags" de este repositorio.

***_No olvides cargar tu archivo csv en la base de datos PostgreSQL._***

### Desarrollar un DAG con procesos ETL (Extract - Trasmform - Load)
El c√≥digo lo puedes encontrar en la carpeta dags de este repositorio. 

<h3 align="left">1. Importaci√≥n de paquetes</h3>

Se importan los paquetes necesarios para el funcionamiento del DAG, como `DAG`, `PythonOperator`, `BranchPythonOperator`, `datetime`, `pandas`, `psycopg2` y `numpy`.

<h3 align= "left"> 2. Extracci√≥n de la data </h3>

La funci√≥n **extract_data()** utiliza la biblioteca `psycopg2` para establecer una conexi√≥n con la base de datos. Se especifican los detalles de conexi√≥n, como el host, el puerto, el nombre de usuario, la contrase√±a y la base de datos a la que se va a acceder.

Una vez que se han obtenido los datos, se cierran el cursor y la conexi√≥n a la base de datos, y los datos extra√≠dos se devuelven.

<h3 align="left"> 3. Transformaci√≥n de los Datos </h3>

La funci√≥n **transform_data()** recibe un par√°metro `task_instance` que representa la instancia de la tarea en Airflow. Utiliza el m√©todo xcom_pull() del objeto task_instance para obtener los datos extra√≠dos anteriormente mediante el uso de task_ids='extract_data'.

Los datos transformados se almacenan en una lista llamada `transformed_data`. A continuaci√≥n, se utiliza el m√©todo xcom_push() del objeto task_instance para guardar los datos transformados con la clave 'transformed_data' en el contexto de XCom, lo que permite compartirlos con otras tareas.

Finalmente, la funci√≥n devuelve la lista transformed_data.

<h3 align="left"> 4. C√°lculo de la Media (proceso de Data Mining)</h3>

La funci√≥n **calculate_mean()** recibe un par√°metro task_instance que representa la instancia de la tarea en Airflow. Utiliza el m√©todo xcom_pull() del objeto task_instance para obtener los datos transformados obtenidos en el paso anterior mediante el uso de task_ids='transform_data' y key='transformed_data'.

Dentro del bucle, se filtran los datos correspondientes a cada grupo y se calcula la media de los casos de violaci√≥n reportados utilizando la funci√≥n np.mean() de la biblioteca numpy.Los resultados se almacenan en un diccionario llamado means_by_group, donde la clave es el nombre del grupo y el valor es la media calculada.

<h3 align="left">5. Carga de los Datos </h3>

La **funci√≥n load_data()** Obtiene los datos transformados (transformed_data) y los resultados del c√°lculo de la media (means_by_group) obtenidos en los pasos anteriores mediante el uso de los par√°metros task_ids y key.

- Se establece una conexi√≥n con la base de datos PostgreSQL utilizando la biblioteca psycopg2.
- Se crea una tabla llamada loaded_data si no existe previamente y se ejecuta mediante el m√©todo execute() del cursor.
- Se insertan los datos en la tabla utilizando una consulta SQL parametrizada.

Una vez que se han insertado todos los datos, se realiza la confirmaci√≥n de la transacci√≥n (conn.commit()) y se cierran el cursor y la conexi√≥n a la base de datos.

<h3 align="left">6. Segmentaci√≥n de los Datos (proceso de Data Mining)</h3>
   
Obtiene los datos extra√≠dos (data) del paso anterior mediante el uso de task_ids='extract_data'.

- Se seleccionan solo las columnas necesarias del resultado mediante una comprensi√≥n de lista. 
- Se extraen los campos 'area_name', 'year', 'subgroup' y 'victims_of_rape_total' de cada fila en data y se almacenan en selected_data.
- Se crea un DataFrame de pandas (df) utilizando selected_data, especificando los nombres de las columnas correspondientes.

Los resultados de ambas segmentaciones se concatenan utilizando el m√©todo concat() de pandas, y se obtiene un DataFrame (segmented_data) con los datos segmentados. Finalmente, se convierte el DataFrame segmented_data en una lista de diccionarios utilizando el m√©todo to_dict('records') de pandas.

<h3 align="left"> 7. Carga de los Datos Segmentados</h3>
   
La funci√≥n **load_segmented_data()** obtiene los datos segmentados (transformed_data) obtenidos en el paso anterior mediante el uso del par√°metro task_ids='segment_data'.

- Se establece una conexi√≥n con la base de datos PostgreSQL utilizando la biblioteca psycopg2.
- Se crea una tabla llamada segmented_data si no existe previamente.
- Se insertan los datos segmentados en la tabla utilizando una consulta SQL parametrizada.

Una vez que se han insertado todos los datos, se realiza la confirmaci√≥n de la transacci√≥n (conn.commit()) y se cierran el cursor y la conexi√≥n a la base de datos. Revisar que la tabla segmented_data se haya creado correctamente en la bd. 

<h3 align="left">8. Configuraci√≥n del DAG</h3>
   
El c√≥digo proporcionado muestra la configuraci√≥n del DAG y la creaci√≥n de tareas utilizando el objeto PythonOperator en Airflow.

- Se define un diccionario `default_args` que contiene los argumentos por defecto para el DAG, como el propietario (owner) y la fecha de inicio (start_date). Estos argumentos se utilizan para establecer las caracter√≠sticas generales del DAG.
- Se crea un objeto DAG llamado etl_dag5 utilizando el constructor DAG de Airflow. Se especifica el nombre del DAG, los argumentos por defecto (default_args), una descripci√≥n y el intervalo de programaci√≥n (schedule_interval) establecido como None en este caso.

<h3 align="left">9. Definici√≥n de las tareas</h3>

Se definen las tareas utilizando objetos `PythonOperator`. Cada tarea tiene un identificador **(task_id)**, una funci√≥n de Python **(python_callable)** que se ejecutar√° como parte de la tarea y el DAG al que pertenece (dag).

Algunas de las tareas, como **transform_task**, **mean_task**, **load_task,** **segment_task** y **load_segment_task**, se definen con el par√°metro `provide_context=True`. Esto permite que la funci√≥n de Python asociada a cada tarea acceda al contexto de Airflow, lo que es √∫til para utilizar variables y resultados de tareas anteriores.

<h3 align="left">10. Configuraci√≥n de las dependecias de las tareas</h3>

Por √∫ltimo, se establecen las dependencias entre las tareas utilizando el operador >>. En este caso, las tareas `extract_task`, `transform_task`, `mean_task`, `load_task` y `segment_task` dependen de la tarea **extract_task**, y la tarea **load_segment_task** depende de la tarea **segment_task**.

<hr> 

# Espero que esta documentaci√≥n sea de gran ayuda üë©‚Äçüíªüíü!!!!! 


